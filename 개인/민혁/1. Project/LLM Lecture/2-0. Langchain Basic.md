# Langchain Basic

> every part will going in virtual env

<hr />

## model

1.  import LLM(Chat Model)

    ```
    from langchain.chat_models import ChatOllama
    ```

    -   u can change another model easily
        -   `... import ChatOllama` -> `... import ChatOpenAI`

    <br>

2.  predict

    ```
    ChatOllama(model = "mistral:latest").predict("tell me about Ollama")
    ```

    -   `model = "mistral:latest"`: select model.

        -   if u want to use `llama2`, `model = "llama2"` (of cource, install llama2 is requirements)

    -   Chat Model can predict 'text' (optimized 'chat')
        it can also not just get a question, they can conversation.

    -   not only model select u can also so many things

        ```
        ChatOllama(model = "mistral:latest", temprature=0.1),
        max_tokens=150,
        ...
        ```

        like this, u can see another things using autocomplete.

<hr/>

## Message Constructor

```
from langchain.schema import HumanMessage, AIMessage, SystemMessage
```

`HumanMessage`: we give message to models
<br>
`AIMessage`: responsed messages
<br>
`SystemMessage`: provided settings to LLM (background context)

<br>

```
message = [
    SystemMessage(content="you are a world class chef"),
    AIMessage(content="hello! i'm chef Paik"),
    HumanMessage(content="give me a recipe milk tea"),
]
```

```
chat_model = ChatOllama(model = "mistral:latest")

chat_model.predict_messages(message)
```

> `predict`: Pass a single string input to the model and return a string prediction.<br>`predict_messages`:Pass a message sequence to the model and return a message prediction.

`message`: you can change name. must be Array<br>
`content`: The string contents of the message.

<br>
<hr />

## More Customazing

### Prompt Template

prompt is important<br>
that is only way communicate with LLM<br>
if u have good prompt, u can get a good response

Langchain have so many utilities to work about Prompt

```
from langchain.prompts import PromptTemplate, ChatPromptTemplate
```

`PromptTemplate`: create template from string<br>
`ChatPromptTemplate`: create template from messages

example P.T:

-   u can use PromptTemplate if u dont want to use `SystemMessage`, `AIMessage`.
-   ```
      template = template = PromptTemplate.from_template("give me recipe {food}")
      print(template.format(food="milk tea"))
    ```
    will be return string. `give me recipe milk tea`<br>
    u can use this like below code<br><br>
    ```
    prompt = template.format(food="milk tea")
    model.predict(prompt)
    ```

example C.P.T

-   u can use messages array easliy
    ```
    template = ChatPromptTemplate.from_messages([
      ("system", "input System Prompt!"),
      ("ai", "imput Ai Prompt!"),
      ("human", "imput {who} Prompt!")
      ])
    ```
    like this.<br>
    of course need to `template.format_messages(who = "Human")`

<br>

you can use C.P or C.P.T, easliy to validate variables or custom

<br>
<hr />
<br>

## Langchain Expression Language

### Chain

`chain`: basically all element together<br>
all going to run as a chain one after to other until we get result

```
chain = template_you_made | model
chain.invoke({})
```

done!<br>
if u have something `{}`, in template follow below code

```
chain.invoke({
    "template_component_num": 10,
    "template_component_str": "input asdff"
})
```

internally, langchain call `.format_messages` for us. then call `chat.predict` for us.<br>
langchain will send each part's output to next part for us!

if u have [output parser](https://python.langchain.com/docs/modules/model_io/output_parsers/), just add parser at code<br>

-   `chain = template | model | parser_you_made`

model's result will send to parser
